{"cells":[{"cell_type":"markdown","metadata":{"id":"zDKlsNTq_4xF"},"source":["# Assignment 4: Simple Sentiment Analysis with PyTorch\n","\n","**Goal:** Your task is to complete the code in this notebook to build and train a basic neural network for binary sentiment classification (positive/negative) using PyTorch.\n","\n","**Instructions:**\n","\n","1.  Read through the explanations in each section carefully.\n","2.  Find the code blocks marked with `# TODO:` or `YOUR CODE HERE`.\n","3.  Fill in the missing code according to the instructions.\n","4.  Run the notebook sequentially. **Do not change the `torch.manual_seed(42)` line**, as it's crucial for auto-scoring.\n","5.  After completing a section, run the corresponding \"✅ Check Your Work\" cell to see if your implementation is correct.\n","6. If you don't get full credit for a block, go back, edit your code in that block, and rerun it (along with any dependent cells) until you pass the check.\n","7.  Your final score will be based on passing these checks.\n","\n","**Let's get started!**"]},{"cell_type":"markdown","metadata":{"id":"bcM0ZtFX_4xG"},"source":["## 1. Setup\n","\n","First, let's import the required PyTorch libraries and other utilities. This part is already done for you.\n","\"\"\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jWUKZssz_4xG","executionInfo":{"status":"ok","timestamp":1749421894432,"user_tz":420,"elapsed":8,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","import os, re, random\n","import numpy as np\n","import torch.nn.functional as F\n","\n","# --- DO NOT CHANGE THIS SEED ---\n","# Set random seed for reproducibility and scoring\n","SEED = 42\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","# -------------------------------"]},{"cell_type":"markdown","metadata":{"id":"bDl0qFUK_4xG"},"source":["## 2. Data\n","\n","We'll use a very small, hardcoded dataset for simplicity.\n","\n","*   `1` represents positive sentiment.\n","*   `0` represents negative sentiment."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WVNF_HCw_4xG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749418889194,"user_tz":420,"elapsed":17,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"a059815e-b744-4e8d-cd26-d5b483fb8ac1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Texts: ['really good movie', 'absolutely loved it', 'such a great film']\n","Sample Labels: [1, 1, 1]\n"]}],"source":["raw_data = [\n","    (\"really good movie\", 1), (\"absolutely loved it\", 1), (\"such a great film\", 1), (\"wonderful and touching\", 1),\n","    (\"incredible story\", 1), (\"very enjoyable experience\", 1), (\"loved every moment\", 1), (\"heartwarming and sweet\", 1),\n","    (\"amazing acting\", 1), (\"fantastic film overall\", 1), (\"excellent direction\", 1), (\"great plot\", 1),\n","    (\"a masterpiece\", 1), (\"beautifully made\", 1), (\"just perfect\", 1), (\"top notch acting\", 1),\n","    (\"brilliant and emotional\", 1), (\"superb movie experience\", 1), (\"highly recommend this\", 1), (\"best film I've seen\", 1),\n","    (\"great visuals\", 1), (\"emotional and inspiring\", 1), (\"outstanding in every way\", 1), (\"so much fun\", 1),\n","    (\"pleasantly surprised\", 1), (\"joyful and uplifting\", 1), (\"flawless performance\", 1), (\"great positive message\", 1),\n","    (\"positive feelings only\", 1), (\"a must watch movie\", 1), (\"truly captivating story\", 1), (\"loved the music\", 1),\n","    (\"looked stunning\", 1), (\"genuinely funny parts\", 1), (\"a feel good movie\", 1), (\"kept me engaged\", 1),\n","    (\"great chemistry between actors\", 1), (\"clever writing\", 1), (\"visually impressive\", 1), (\"exceeded expectations\", 1),\n","    (\"a very satisfying watch\", 1), (\"powerful performances\", 1), (\"unique and original\", 1), (\"well developed story\", 1),\n","    (\"the ending was perfect\", 1), (\"charming and delightful\", 1), (\"a real gem of a film\", 1), (\"thought provoking and deep\", 1),\n","    (\"laugh out loud funny\", 1), (\"skillfully directed\", 1), (\"excellent pacing\", 1), (\"incredibly well acted\", 1),\n","    (\"beautiful musical score\", 1), (\"a heartwarming story\", 1), (\"a visual treat\", 1), (\"genuinely moving\", 1),\n","    (\"great special effects\", 1), (\"loved the dialogue\", 1), (\"memorable characters\", 1), (\"entertaining from start to finish\", 1),\n","    (\"a fantastic journey\", 1), (\"will watch it again\", 1), (\"highly imaginative plot\", 1), (\"superbly crafted\", 1),\n","    (\"full of charm\", 1), (\"refreshingly different\", 1), (\"strong emotional connection\", 1), (\"genuinely touching moments\", 1),\n","    (\"brilliant execution\", 1), (\"a delightful surprise\", 1), (\"outstanding writing\", 1), (\"perfect casting\", 1),\n","    (\"loved the overall atmosphere\", 1), (\"truly inspiring\", 1), (\"masterful storytelling\", 1), (\"an absolute joy to watch\", 1),\n","    (\"well worth the time\", 1), (\"great character development\", 1),\n","\n","    (\"really bad movie\", 0), (\"absolutely hated it\", 0), (\"such a boring film\", 0), (\"terrible and slow\", 0),\n","    (\"weak story\", 0), (\"very disappointing experience\", 0), (\"hated every moment\", 0), (\"cringe worthy and dull\", 0),\n","    (\"awful acting\", 0), (\"horrible film overall\", 0), (\"poor direction\", 0), (\"bad plot\", 0),\n","    (\"a total disaster\", 0), (\"badly made movie\", 0), (\"just plain awful\", 0), (\"low quality acting\", 0),\n","    (\"stupid and annoying\", 0), (\"painful movie experience\", 0), (\"do not recommend this\", 0), (\"worst film I've seen\", 0),\n","    (\"ugly visuals\", 0), (\"dry and boring\", 0), (\"complete waste of time\", 0), (\"so much cringe\", 0),\n","    (\"deeply disappointed\", 0), (\"frustrating and flat\", 0), (\"terrible negative message\", 0), (\"negative feelings only\", 0),\n","    (\"a must skip movie\", 0), (\"not worth watching\", 0), (\"truly dreadful story\", 0), (\"hated the music\", 0),\n","    (\"looked jarring\", 0), (\"genuinely unfunny parts\", 0), (\"a depressing movie\", 0), (\"lost my interest\", 0),\n","    (\"zero chemistry between actors\", 0), (\"lazy writing\", 0), (\"visually unappealing\", 0), (\"failed expectations\", 0),\n","    (\"a very unsatisfying watch\", 0), (\"weak performances\", 0), (\"unoriginal and derivative\", 0), (\"poorly developed story\", 0),\n","    (\"the ending was terrible\", 0), (\"awkward and unpleasant\", 0), (\"a real dud of a film\", 0), (\"nonsensical and confusing\", 0),\n","    (\"trying too hard\", 0), (\"clumsily directed\", 0), (\"terrible pacing\", 0), (\"incredibly poorly acted\", 0),\n","    (\"annoying musical score\", 0), (\"a tedious story\", 0), (\"an ugly mess\", 0), (\"genuinely irritating\", 0),\n","    (\"bad special effects\", 0), (\"hated the dialogue\", 0), (\"forgettable characters\", 0), (\"boring from start to finish\", 0),\n","    (\"a painful journey\", 0), (\"will avoid watching again\", 0), (\"completely unimaginative plot\", 0), (\"poorly crafted\", 0),\n","    (\"lacked any charm\", 0), (\"predictable and stale\", 0), (\"no emotional depth\", 0), (\"forced and fake moments\", 0),\n","    (\"terrible execution\", 0), (\"an unpleasant surprise\", 0), (\"awful writing\", 0), (\"terrible casting\", 0),\n","    (\"hated the overall atmosphere\", 0), (\"truly pointless\", 0), (\"confusing storytelling\", 0), (\"an absolute pain to watch\", 0),\n","    (\"not worth the money\", 0), (\"weak character development\", 0)\n","]\n","\n","# Separate texts and labels - Provided\n","texts = [text for text, label in raw_data]\n","labels = [label for text, label in raw_data]\n","\n","print(\"Sample Texts:\", texts[:3])\n","print(\"Sample Labels:\", labels[:3])"]},{"cell_type":"markdown","metadata":{"id":"l4tWTYR0_4xG"},"source":["## 3. Preprocessing\n","\n","Text data needs to be converted into numbers that our neural network can understand. This section is mostly provided.\n","\n","### 3.1. Tokenization and Cleaning"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0Almmjq1_4xG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749418906827,"user_tz":420,"elapsed":50,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"c94ef7ea-3e46-4b98-89bc-43639c02bb0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Sample: ['really', 'good', 'movie']\n"]}],"source":["# Provided\n","def simple_tokenizer(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-z\\s]\", \"\", text) # Keep only letters and spaces\n","    tokens = text.split()\n","    return tokens\n","\n","# Tokenize all texts - Provided\n","tokenized_texts = [simple_tokenizer(text) for text in texts]\n","print(\"Tokenized Sample:\", tokenized_texts[0])"]},{"cell_type":"markdown","metadata":{"id":"AxW5H__3_4xG"},"source":["### 3.2. Build Vocabulary\n","\n","Create a mapping from unique words to integer indices. We'll add special tokens: `<PAD>` for padding (though `EmbeddingBag` handles variable lengths nicely) and `<UNK>` for unknown words encountered later."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"nspolFNg_4xG","executionInfo":{"status":"ok","timestamp":1749418909257,"user_tz":420,"elapsed":8,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fafec29b-9580-42bd-9a31-d390725ec304"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary Size: 234\n","Sample Vocab: [('<PAD>', 0), ('<UNK>', 1), ('really', 2), ('good', 3), ('movie', 4), ('absolutely', 5), ('loved', 6), ('it', 7), ('such', 8), ('a', 9)]\n"]}],"source":["# Provided\n","# Count word frequencies\n","word_counts = Counter(token for text in tokenized_texts for token in text)\n","\n","# Create vocabulary (mapping word to index)\n","vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n","vocab.update({word: i+2 for i, (word, count) in enumerate(word_counts.items())})\n","\n","vocab_size = len(vocab)\n","print(\"Vocabulary Size:\", vocab_size)\n","print(\"Sample Vocab:\", list(vocab.items())[:10])"]},{"cell_type":"markdown","metadata":{"id":"1Ly2V5p1_4xH"},"source":["### 3.3. Numericalize Text\n","\n","Convert each tokenized sentence into a sequence of integers using the vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daU-3DAq_4xH"},"outputs":[],"source":["# Provided\n","def numericalize(tokens, vocab):\n","    return [vocab.get(token, 1) for token in tokens] # Use <UNK> index (1) if word is not in vocab\n","\n","numericalized_texts = [numericalize(tokens, vocab) for tokens in tokenized_texts]\n","print(\"Numericalized Sample:\", numericalized_texts[0])"]},{"cell_type":"markdown","metadata":{"id":"npWpqVYy_4xH"},"source":["### 3.4. Encode Labels\n","\n","Our labels (0 and 1) are already numerical, but we'll convert them to tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4qEWDkG_4xH"},"outputs":[],"source":["# Provided\n","# Convert labels to tensors\n","encoded_labels = torch.tensor(labels, dtype=torch.float32) # Use float for BCEWithLogitsLoss\n","print(\"Encoded Labels Sample:\", encoded_labels[:3])"]},{"cell_type":"markdown","metadata":{"id":"dc6k9yZg_4xH"},"source":["## 4. Dataset & DataLoader\n","\n","We need a way to efficiently load and batch our data during training. This is provided for you.\n","\n","*   **`SentimentDataset`**: A custom class that holds our numericalized texts and labels.\n","*   **`collate_fn`**: A function used by the `DataLoader` to process a batch of data points. It handles sequences of varying lengths and prepares them for the `EmbeddingBag` layer by creating offsets.\n","*   **`DataLoader`**: Manages batching and shuffling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9N0vxYLH_4xH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748814991216,"user_tz":420,"elapsed":8,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"84cb3fb2-db38-456f-e41a-86795872b40a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Sample Batch --- \n","Text Tensor:  tensor([  9,  17,  89,  66,  49,  84,  39,  13,  48, 206,  95,  16])\n","Labels Tensor: tensor([1., 1., 1., 0.])\n","Offsets Tensor: tensor([0, 4, 2, 3])\n","--------------------\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-34-f9a31910404c>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  processed_text = torch.tensor(_text, dtype=torch.int64)\n"]}],"source":["# Provided\n","class SentimentDataset(Dataset):\n","    def __init__(self, numericalized_texts, labels):\n","        self.texts = numericalized_texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        # Return text as a tensor\n","        return torch.tensor(self.texts[idx], dtype=torch.int64), self.labels[idx]\n","\n","# Provided\n","# Collate function to handle variable length sequences for EmbeddingBag\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_text, _label) in batch:\n","        label_list.append(_label)\n","        processed_text = torch.tensor(_text, dtype=torch.int64)\n","        text_list.append(processed_text)\n","        # Track the starting index of each sequence in the concatenated tensor\n","        offsets.append(processed_text.size(0))\n","\n","    label_list = torch.tensor(label_list, dtype=torch.float32)\n","    # Concatenate all sequences into a single tensor\n","    text_list = torch.cat(text_list)\n","\n","    # Convert offsets to a tensor; remove the last element as it's the total length\n","    offsets = torch.tensor(offsets[:-1], dtype=torch.int64)\n","    return text_list, label_list, offsets\n","\n","# Create the dataset instance\n","dataset = SentimentDataset(numericalized_texts, encoded_labels)\n","\n","# Create the DataLoader\n","batch_size = 4 # Small batch size for our small dataset\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_batch,\n","    worker_init_fn=np.random.seed(SEED) # Ensure dataloader shuffle is deterministic\n",")\n","\n","# Example of one batch (optional check)\n","try:\n","  text_batch, label_batch, offset_batch = next(iter(dataloader))\n","  print(\"--- Sample Batch --- \")\n","  print(\"Text Tensor: \", text_batch)\n","  print(\"Labels Tensor:\", label_batch)\n","  print(\"Offsets Tensor:\", offset_batch)\n","  print(\"--------------------\")\n","except StopIteration:\n","  print(\"Dataloader is empty or batch size is larger than dataset size.\")"]},{"cell_type":"markdown","source":["## 5. Model Definition (70 Points)\n","\n","**Task 1:** Define the neural network architecture (**40 points**).\n","\n","*   You need to initialize the layers in the `__init__` method:\n","    *   An `nn.EmbeddingBag` layer. Use the provided `vocab_size` and `embed_dim`. Set `sparse=False` and `mode=\"mean\"`.\n","    *   A `nn.Linear` layer for the hidden layer. It should map from `embed_dim` to `hidden_dim`.\n","    *   An `nn.Dropout` layer. Use the provided `dropout` rate.\n","    *   A final `nn.Linear` layer for the output. It should map from `hidden_dim` to `num_class`.\n","*   You need to implement the `forward` method (**30 points**):\n","    *   Pass the `text` and `offsets` through the `embedding` layer.\n","    *   Pass the result through the `hidden` layer, followed by a `ReLU` activation function (`F.relu`).\n","    *   Apply `dropout` to the hidden layer's output.\n","    *   Pass the result through the `output` layer.\n","    *   Return the final output (these will be logits)."],"metadata":{"id":"qcUCnUGh3-he"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TuH20Ds_4xH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748814991229,"user_tz":420,"elapsed":10,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"66cc17c4-4cf8-4326-a7fb-efe799bb8fbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Architecture:\n","SentimentNet(\n","  (embedding): EmbeddingBag(234, 256, mode='mean')\n","  (hidden): Linear(in_features=256, out_features=256, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (output): Linear(in_features=256, out_features=1, bias=True)\n",")\n"]}],"source":["class SentimentNet(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class, dropout=0.5):\n","        super(SentimentNet, self).__init__()\n","        self.embedding = None  # TODO: Initialize the nn.EmbeddingBag layer (10 Point)\n","        self.hidden = None    # TODO: Initialize the first nn.Linear hidden layer (10 Point)\n","        self.dropout = None   # TODO: Initialize the nn.Dropout layer (10 Point)\n","        self.output = None    # TODO: Initialize the final nn.Linear output layer (10 Point)\n","\n","        # --- YOUR CODE HERE (~4 Lines) ---\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode=\"mean\") # YOUR CODE HERE\n","        self.hidden = nn.Linear(embed_dim, hidden_dim) # YOUR CODE HERE\n","        self.dropout = nn.Dropout(p=dropout) # YOUR CODE HERE\n","        self.output = nn.Linear(hidden_dim, num_class) # YOUR CODE HERE\n","        # --- END YOUR CODE ---\n","\n","\n","    def forward(self, text, offsets):\n","        embedded = None # Placeholder\n","        hidden_out = None # Placeholder\n","        output = None # Placeholder\n","\n","        # --- YOUR CODE HERE (Implement forward pass) (~4 Lines, 30 points) ---\n","        # 1. Apply embedding layer\n","        embedded = self.embedding(text, offsets) # YOUR CODE HERE\n","\n","        # 2. Pass through hidden layer and ReLU activation\n","        hidden_out = F.relu(self.hidden(embedded)) # YOUR CODE HERE\n","\n","        # 3. Apply dropout\n","        hidden_out = self.dropout(hidden_out) # YOUR CODE HERE\n","\n","        # 4. Pass through the output layer\n","        output = self.output(hidden_out)  # YOUR CODE HERE\n","        # --- END YOUR CODE ---\n","\n","        return output\n","\n","# Model Hyperparameters (Provided)\n","EMBED_DIM = 256\n","HIDDEN_DIM = 256\n","NUM_CLASS = 1 # For binary classification\n","DROP_RATE = 0.2\n","\n","# Instantiate the model (using your definition above)\n","model = SentimentNet(vocab_size, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, num_class=NUM_CLASS, dropout=DROP_RATE)\n","print(\"Model Architecture:\")\n","\n","print(model)"]},{"cell_type":"markdown","source":["### ✅ Check Your Work: Task 1 - Model Definition"],"metadata":{"id":"Lau8uHc04S9x"}},{"cell_type":"code","source":["# Check Task 1 (Model Definition)\n","points_task1 = 0\n","target_output_shape = torch.Size([batch_size, NUM_CLASS]) # Expect (batch_size, 1)\n","try:\n","    # Get a sample batch\n","    torch.manual_seed(SEED) # Reset seed just before getting batch\n","    dataloader_iter = iter(DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, worker_init_fn=np.random.seed(SEED)))\n","    text_batch, _, offset_batch = next(dataloader_iter)\n","\n","    # Check if layers are initialized\n","    if isinstance(model.embedding, nn.EmbeddingBag) and \\\n","       isinstance(model.hidden, nn.Linear) and \\\n","       isinstance(model.dropout, nn.Dropout) and \\\n","       isinstance(model.output, nn.Linear):\n","       points_task1 += 40 # 10 point per layer init\n","       print(\"[Task 1] Layer Initialization: Correct! (+40 points)\")\n","    else:\n","       print(\"[Task 1] Layer Initialization: Incorrect. Check nn.EmbeddingBag, nn.Linear, nn.Dropout initializations.\")\n","\n","    # Check forward pass output shape\n","    model.eval() # Set to eval mode for checking\n","    with torch.no_grad():\n","        output = model(text_batch, offset_batch)\n","        if output.shape == target_output_shape:\n","             points_task1 += 30\n","             print(f\"[Task 1] Forward Pass Output Shape: Correct! ({output.shape}) (+30 point)\")\n","        else:\n","             print(f\"[Task 1] Forward Pass Output Shape: Incorrect. Expected {target_output_shape}, got {output.shape}.\")\n","\n","except Exception as e:\n","    print(f\"[Task 1] Error during check: {e}\")\n","    print(\"Make sure your layer dimensions and forward pass logic are correct.\")"],"metadata":{"id":"CyZb8G944hUu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748814991261,"user_tz":420,"elapsed":29,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"96666bdf-b730-4910-9643-b1e27a75bc74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Task 1] Layer Initialization: Correct! (+40 points)\n","[Task 1] Forward Pass Output Shape: Correct! (torch.Size([4, 1])) (+30 point)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-34-f9a31910404c>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  processed_text = torch.tensor(_text, dtype=torch.int64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"u2h9BiGI_4xH"},"source":["## 6. Training Setup\n","\n","**Task 2:** Set up the loss function and optimizer. **(20 Points Total)**\n","\n","*   Instantiate the loss function: Use `nn.BCEWithLogitsLoss`, which is suitable for binary classification with logits output. (10 Point)\n","*   Instantiate the optimizer: Use `optim.Adam`, passing the `model.parameters()` and the specified `learning_rate`. (10 Point)"]},{"cell_type":"code","source":["# Training Hyperparameters (Provided)\n","learning_rate = 1e-4\n","num_epochs = 100\n","\n","# --- YOUR CODE HERE (~2 Lines)---\n","criterion = nn.BCEWithLogitsLoss() # TODO: Define the loss function (Use BCEWithLogitsLoss) (10 Point)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate) # TODO: Define the optimizer (Use Adam) (10 Point)\n","# --- END YOUR CODE ---"],"metadata":{"id":"zMMrkX3l5tcq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✅ Check Your Work: Task 2 - Loss & Optimizer"],"metadata":{"id":"S3RgHbna5zj3"}},{"cell_type":"code","source":["# Check Task 2 (Loss and Optimizer)\n","points_task2 = 0\n","try:\n","    if isinstance(criterion, nn.BCEWithLogitsLoss):\n","        points_task2 += 10\n","        print(\"[Task 2] Criterion Definition: Correct! (+10 point)\")\n","    else:\n","        print(\"[Task 2] Criterion Definition: Incorrect. Should be nn.BCEWithLogitsLoss.\")\n","\n","    if isinstance(optimizer, optim.Adam):\n","         if len(optimizer.param_groups) > 0 and len(optimizer.param_groups[0]['params']) > 0:\n","             points_task2 += 10\n","             print(\"[Task 2] Optimizer Definition: Correct! (+10 point)\")\n","         else:\n","             print(\"[Task 2] Optimizer Definition: Potentially incorrect. Did you pass model.parameters()?\")\n","    else:\n","         print(\"[Task 2] Optimizer Definition: Incorrect. Should be optim.Adam.\")\n","\n","except Exception as e:\n","    print(f\"[Task 2] Error during check: {e}\")"],"metadata":{"id":"uEF2m8mk55W6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748814991289,"user_tz":420,"elapsed":15,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"50059502-4e49-4060-a6b4-d4c1ace268ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Task 2] Criterion Definition: Correct! (+10 point)\n","[Task 2] Optimizer Definition: Correct! (+10 point)\n"]}]},{"cell_type":"markdown","source":["## 7. Training Loop (Provided)\n","\n","This section contains the complete training loop. Read through it to understand the process. No tasks here."],"metadata":{"id":"qhllydfN5eXF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gefeXMEq_4xH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748815001019,"user_tz":420,"elapsed":9307,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"40b31301-23eb-4c8b-c532-4917232413b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting Training...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-34-f9a31910404c>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  processed_text = torch.tensor(_text, dtype=torch.int64)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss: 0.6956, Accuracy: 0.4615\n","Epoch [5/100], Loss: 0.6750, Accuracy: 0.5513\n","Epoch [10/100], Loss: 0.6737, Accuracy: 0.6090\n","Epoch [15/100], Loss: 0.6428, Accuracy: 0.6346\n","Epoch [20/100], Loss: 0.6357, Accuracy: 0.6667\n","Epoch [25/100], Loss: 0.6114, Accuracy: 0.6538\n","Epoch [30/100], Loss: 0.6492, Accuracy: 0.6026\n","Epoch [35/100], Loss: 0.6224, Accuracy: 0.6410\n","Epoch [40/100], Loss: 0.6079, Accuracy: 0.6667\n","Epoch [45/100], Loss: 0.5790, Accuracy: 0.6603\n","Epoch [50/100], Loss: 0.5939, Accuracy: 0.6218\n","Epoch [55/100], Loss: 0.5927, Accuracy: 0.6538\n","Epoch [60/100], Loss: 0.5542, Accuracy: 0.6987\n","Epoch [65/100], Loss: 0.5742, Accuracy: 0.7051\n","Epoch [70/100], Loss: 0.5543, Accuracy: 0.6538\n","Epoch [75/100], Loss: 0.5753, Accuracy: 0.6859\n","Epoch [80/100], Loss: 0.5741, Accuracy: 0.6603\n","Epoch [85/100], Loss: 0.5078, Accuracy: 0.6667\n","Epoch [90/100], Loss: 0.5241, Accuracy: 0.6859\n","Epoch [95/100], Loss: 0.5082, Accuracy: 0.6795\n","Epoch [100/100], Loss: 0.5061, Accuracy: 0.6859\n","\n","Training finished.\n"]}],"source":["print(\"\\nStarting Training...\")\n","# Ensure model, criterion, optimizer are defined from previous steps\n","if 'model' not in locals() or model is None or criterion is None or optimizer is None:\n","     print(\"ERROR: Model, criterion, or optimizer not defined. Please complete previous steps.\")\n","else:\n","    # Make sure the optimizer is linked to the *current* model instance from Task 1 check reset\n","    # If Task 2 was correct, this re-links the same type of optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    torch.manual_seed(SEED) # Ensure consistent training start\n","\n","    training_losses = []\n","    final_epoch_acc = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train() # Set model to training mode\n","        total_loss = 0\n","        total_correct = 0\n","        total_samples = 0\n","\n","        # We rely on the DataLoader's shuffle seeded correctly at initialization\n","        for texts_batch, labels_batch, offsets_batch in dataloader:\n","\n","            # Skip batch if texts_batch is empty\n","            if texts_batch.numel() == 0: continue\n","\n","            # Zero gradients\n","            optimizer.zero_grad()\n","\n","            # --- Training Steps ---\n","            # 1. Forward pass\n","            outputs = model(texts_batch, offsets_batch) # Shape: (batch_size, 1)\n","\n","            # 2. Squeeze output for the loss function\n","            outputs_squeezed = outputs.squeeze(1) # Shape: (batch_size)\n","\n","            # 3. Calculate loss\n","            loss = criterion(outputs_squeezed, labels_batch)\n","\n","            # 4. Backward pass\n","            loss.backward()\n","\n","            # 5. Optimize\n","            optimizer.step()\n","\n","            # 6. Calculate accuracy\n","            predicted = (torch.sigmoid(outputs_squeezed) > 0.5).float()\n","            # --- End Training Steps ---\n","\n","            # Track metrics\n","            total_loss += loss.item() * labels_batch.size(0)\n","            total_correct += (predicted == labels_batch).sum().item()\n","            total_samples += labels_batch.size(0)\n","\n","        # Calculate average loss and accuracy for the epoch\n","        if total_samples > 0:\n","            epoch_loss = total_loss / total_samples\n","            epoch_acc = total_correct / total_samples\n","            training_losses.append(epoch_loss)\n","            final_epoch_acc = epoch_acc\n","        else:\n","            epoch_loss = 0\n","            epoch_acc = 0\n","\n","        if (epoch + 1) % 5 == 0 or epoch == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n","\n","    print(\"\\nTraining finished.\")"]},{"cell_type":"markdown","metadata":{"id":"LcDmTbid_4xH"},"source":["## 8. Evaluation (Provided)\n","\n","Evaluate your trained model on unseen test data. No tasks here, just run the cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buP4yQVK_4xH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748815001048,"user_tz":420,"elapsed":17,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"af8997fc-fa25-4e2b-ce6d-0a6be0a40b05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation on training data: Loss: 0.5160, Accuracy: 0.6071\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-34-f9a31910404c>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  processed_text = torch.tensor(_text, dtype=torch.int64)\n"]}],"source":["def evaluate(model, dataloader, criterion):\n","    model.eval() # Set model to evaluation mode\n","    total_loss = 0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    with torch.no_grad(): # Disable gradient calculation\n","        for texts_batch, labels_batch, offsets_batch in dataloader:\n","\n","            outputs = model(texts_batch, offsets_batch).squeeze(1)\n","            loss = criterion(outputs, labels_batch)\n","\n","            total_loss += loss.item() * labels_batch.size(0)\n","            predicted = (torch.sigmoid(outputs) > 0.5).float()\n","            total_correct += (predicted == labels_batch).sum().item()\n","            total_samples += labels_batch.size(0)\n","\n","    if total_samples > 0:\n","        avg_loss = total_loss / total_samples\n","        avg_acc = total_correct / total_samples\n","        print(f'Evaluation on training data: Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}')\n","    else:\n","        print(\"Evaluation failed: No samples processed.\")\n","\n","# Define test examples\n","test_sentences = [\n","    \"strong emotional core\",\n","    \"genuinely touching moments\",\n","    \"brilliant execution\",\n","    \"a delightful surprise\",\n","    \"outstanding screenplay\",\n","    \"perfect casting choice\",\n","    \"loved the atmosphere\",\n","    \"truly inspiring message\",\n","    \"masterful storytelling\",\n","    \"an absolute joy\",\n","    \"well worth the time\",\n","    \"great character arcs\",\n","    \"the music was perfect\",\n","    \"fantastic world building\",\n","\n","    \"no emotional core\",\n","    \"forced and fake moments\",\n","    \"terrible execution\",\n","    \"an unpleasant surprise\",\n","    \"awful screenplay\",\n","    \"terrible casting choice\",\n","    \"hated the atmosphere\",\n","    \"truly pointless message\",\n","    \"confusing storytelling\",\n","    \"an absolute pain\",\n","    \"not worth the money\",\n","    \"weak character arcs\",\n","    \"the music was awful\",\n","    \"lazy world building\",\n","]\n","test_labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","# Tokenize and numericalize\n","test_tokenized = [simple_tokenizer(sent) for sent in test_sentences]\n","test_numericalized = [numericalize(tokens, vocab) for tokens in test_tokenized]\n","test_encoded_labels = torch.tensor(test_labels, dtype=torch.float32)\n","\n","# Create test dataset and dataloader\n","test_dataset = SentimentDataset(test_numericalized, test_encoded_labels)\n","test_loader = DataLoader(test_dataset, batch_size=4, collate_fn=collate_batch)\n","\n","# Evaluate the trained model\n","evaluate(model, test_loader, criterion)"]},{"cell_type":"markdown","metadata":{"id":"M_Rtm4k3_4xH"},"source":["## 9. Prediction Function\n","\n","Let's create a function to predict the sentiment of a new, unseen sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDJxh_F__4xH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748815001064,"user_tz":420,"elapsed":13,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"b0721e75-14f2-4620-a9c1-2effd055576f"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n","Sentence: 'An excellent movie.'\n","Prediction: Positive (Probability: 0.5052)\n","------------------------------\n","Sentence: 'That was a total waste of time.'\n","Prediction: Negative (Probability: 0.6919)\n","------------------------------\n","Sentence: 'Mediocre plot but decent acting.'\n","Prediction: Negative (Probability: 0.7711)\n","------------------------------\n","Sentence: 'Absolutely loved every second!'\n","Prediction: Positive (Probability: 0.6992)\n","------------------------------\n","Sentence: 'The acting was poor and the story made no sense.'\n","Prediction: Negative (Probability: 0.6228)\n","------------------------------\n"]}],"source":["def predict_sentiment(text, model, vocab, tokenizer_func):\n","    model.eval() # Set model to evaluation mod\n","    # 1. Tokenize and numericalize\n","    tokens = tokenizer_func(text)\n","    numericalized = numericalize(tokens, vocab)\n","\n","    # 2. Convert to tensor\n","    text_tensor = torch.tensor(numericalized, dtype=torch.int64)\n","\n","    # 3. Create offsets tensor for a single sequence (starts at 0)\n","    # This is crucial for EmbeddingBag when processing a single item\n","    offsets = torch.tensor([0], dtype=torch.int64)\n","\n","    # 4. Make prediction\n","    with torch.no_grad():\n","        # The model expects batch dimension implicitly via offsets for EmbeddingBag\n","        output = model(text_tensor, offsets)\n","        # Apply sigmoid to get probability\n","        probability = torch.sigmoid(output).item()\n","\n","    # 5. Interpret result\n","    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n","    probability = probability if sentiment == \"Positive\" else 1 - probability\n","    return f\"{sentiment} (Probability: {probability:.4f})\"\n","\n","# --- Test the prediction function ---\n","print(\"-\" * 30)\n","test_sentence1 = \"An excellent movie.\"\n","print(f\"Sentence: '{test_sentence1}'\")\n","print(f\"Prediction: {predict_sentiment(test_sentence1, model, vocab, simple_tokenizer)}\")\n","print(\"-\" * 30)\n","\n","test_sentence2 = \"That was a total waste of time.\"\n","print(f\"Sentence: '{test_sentence2}'\")\n","print(f\"Prediction: {predict_sentiment(test_sentence2, model, vocab, simple_tokenizer)}\")\n","print(\"-\" * 30)\n","\n","test_sentence3 = \"Mediocre plot but decent acting.\"\n","print(f\"Sentence: '{test_sentence3}'\")\n","print(f\"Prediction: {predict_sentiment(test_sentence3, model, vocab, simple_tokenizer)}\")\n","print(\"-\" * 30)\n","\n","test_sentence4 = \"Absolutely loved every second!\"\n","print(f\"Sentence: '{test_sentence4}'\")\n","print(f\"Prediction: {predict_sentiment(test_sentence4, model, vocab, simple_tokenizer)}\")\n","print(\"-\" * 30)\n","\n","test_sentence5 = \"The acting was poor and the story made no sense.\"\n","print(f\"Sentence: '{test_sentence5}'\")\n","print(f\"Prediction: {predict_sentiment(test_sentence5, model, vocab, simple_tokenizer)}\")\n","print(\"-\" * 30)"]},{"cell_type":"markdown","source":["## 🏁 Total Score"],"metadata":{"id":"KbklVhieFn7B"}},{"cell_type":"code","source":["assignment_score = points_task1 + points_task2\n","TOTAL_POINTS = 100\n","print(\"\\n\" + \"=\"*30)\n","print(f\"🏁 Your final score: {assignment_score:.1f} / {TOTAL_POINTS}\")"],"metadata":{"id":"-cRV0aXFB9fQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748815001075,"user_tz":420,"elapsed":9,"user":{"displayName":"Geneviève Connolly","userId":"07039141873206074773"}},"outputId":"52f9dd65-0433-4aff-ae72-843925ced9d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================\n","🏁 Your final score: 90.0 / 100\n"]}]},{"cell_type":"markdown","source":["## Task 3: Experiment with Learning Rate (10 Points)\n","You’ve reached the final question! To receive full credit (**10 points remaining**), you need to experiment with the learning rate. Please go back to the model definition block and rerun it (to re-initialize your model). Then, run all subsequent code blocks up to the learning rate definition block. **Change the learning rate to `1e-3`**, and rerun the optimizer, training, and evaluation blocks using this new learning rate.  \n","\n","After training, report the following results below: **Train Loss**, **Train Accuracy**, **Test Loss**, **Test Accuracy**  \n","\n","**Instructions:**\n","1. Re-initialize your model by rerunning the model definition block.\n","2. In the learning rate definition block, set `learning_rate = 1e-3`.\n","3. Rerun the optimizer, loss function, training loop, and evaluation code.\n","4. Fill in your results above.\n","\n","---\n","\n","**Tip:**  \n","If you make a mistake or want to try again, you can always rerun the relevant blocks as many times as needed to improve your results!"],"metadata":{"id":"qRWy8Q0nwUdI"}},{"cell_type":"code","source":["train_loss = 0.5136  # YOUR CODE HERE\n","train_accuracy = 0.6859  # YOUR CODE HERE\n","test_loss = 0.5019  # YOUR CODE HERE\n","test_accuracy = 0.6071  # YOUR CODE HERE"],"metadata":{"id":"SBq2mC-XwUzx"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[{"file_id":"1VODDWUkRBvLG3Mb-8KhQ1VG5VVyEK-Q7","timestamp":1748632975097},{"file_id":"1UCwLjdMSiV9vfVluvfhof2Z0iMrzEzbF","timestamp":1744976652754}]}},"nbformat":4,"nbformat_minor":0}